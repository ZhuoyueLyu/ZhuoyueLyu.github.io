<html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <meta http-equiv="refresh" content="0; url=http://www.google.com/" /> -->
    <script>
        if (window.location.pathname == "/cs.html") {
            window.location.href = '/?target=cs';
        }
    </script>
</head>

<body>
    <div class=" w3-container w3-white w3-center w3-text-black w3-animate-opacity w3-padding-16"
        style="max-height:100%">
        <div class="w3-content w3-justify " style="max-width:85%">
            <div id="sensing-AI">
                <!-- Title row -->
                <span class="w3-tag w3-blue">Research</span>
                <div class="w3-row">
                    <!-- <span class="w3-tag w3-blue">Author</span> -->
                    <h2 class="secTitle "><strong>Sensing AI</strong><span class="secTitle2 w3-opacity"> | In
                            Progress</span></h2>
                    <h2 class="subTitleCS"><strong class=" w3-text-blue">Zhuoyue Lyu</strong><span class="w3-opacity"> |
                            Aiming at <strong>International Computer
                                Music Conference (ICMC) 2021</strong></span></h2>
                </div>
                <!-- video row -->
                <div class="w3-row">
                    <div class="w3-twothird" style="margin-top: 1rem;">
                        <div class="video_wrapper w3-card-4 ">
                            <iframe class="iframe " src="https://www.youtube.com/embed/P0ebqCZXnow" frameborder="0 "
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture "
                                allowfullscreen></iframe>
                        </div>
                    </div>
                    <div class="w3-third">
                        <div class="text_wrapper">
                            <div class="bodyText">
                                <a>Researchers nowadays mainly focus on the result of AI: the accuracy, the outcome.
                                    Not many of them focus on the <strong>learning process</strong> of AI, i.e., how
                                    does AI "learn"
                                    those things? How does the learning process look like, sound like, and feel like? To
                                    provide people with auditory and visual intuition about AI, I sonified a neural
                                    network by mapping loss and accuracy to oscillators' frequencies and visualized it
                                    using an interactive force-directed graph in Virtual Reality (VR).</a>
                                <p></p>
                                <p>Different colors represent different layers of the neural net. For example, the one
                                    in
                                    the video uses 16 neurons in the first hidden layer (<a class="w3-text-yellow"
                                        style="pointer-events: none;">yellow</a>), 32 in the second hidden layer (<a
                                        class="w3-text-blue" style="pointer-events: none;">blue</a>). The input and
                                    output
                                    are in <a class="w3-text-red" style="pointer-events: none;">red</a> and <a
                                        class="w3-text-green" style="pointer-events: none;">green</a>, respectively. The
                                    white lines between each neuron is mapped to the weight between them; the smaller
                                    the
                                    weight, the more transparent. </p>
                                <p>When the training start, those sounds represent the loss (left channel) and accuracy
                                    (right channel) of each training step. The neurons are jumping around because their
                                    weights in the force-directed graph are being updated.</p>
                                <p>Under the hood, a terminal running a python script is being used to send the
                                    loss/accuracy/weights to Unity through TCP over localhost, which then mapped data to
                                    frequency and updates the visualization.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="terminator-hands" style="margin-top: 7rem;">
                <!-- Title row -->
                <span class="w3-tag w3-blue">Research</span>
                <div class="w3-row">
                    <h2 class="secTitle "><strong>Terminator Hands</strong><span class="secTitle2 w3-opacity"> | In
                            Progress</span></h2>
                    <h2 class="subTitleCS ">Fengyuan Zhu, <strong class=" w3-text-blue">Zhuoyue
                            Lyu</strong>, Tovi Grossman<span class="w3-opacity"> | Aiming at
                            <strong>ACM Symposium on User Interface Software and Technology (UIST) 2021</strong></span>
                    </h2>
                </div>
                <!-- video row -->
                <div class="w3-row">
                    <div class="w3-twothird" style="margin-top: 1rem;">
                        <div id="terminatorFrame" class="video_wrapper w3-card-4 ">
                            <iframe class="iframe " src="https://www.youtube.com/embed/9NTq-GkOQF0" frameborder="0 "
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture "
                                allowfullscreen></iframe>
                        </div>
                    </div>
                    <div id="terminatorText" class="w3-third bodyText" style="margin-top: 1rem;">
                        <a>In Virtual Reality (VR), anything can take any shape. We are interested in how the different
                            hand shapes would impact the interaction between humans and VR objects. As the co-author of
                            this project, I develop four types of morphing hands:</a>
                        <p> 1, 2) Scalable hands: The hand's size is changeable by adjusting the slider on the wrist.
                            Thus, the user can use the appropriate hand size for a specific object, which makes grabbing
                            much easier. Auto-scalable hands: it's based on scalable hands, but the size of the hand is
                            automatically determined by the size of the target object.</p>

                        <p>3) Duplicable hands: This one is my favorite; the user can create multiple copies of the
                            same hand by clicking the controller's button. And the user can adjust the positions and
                            movements of those hands both individually and collectively. This allows the user to
                            grab/operate multiple objects at the same time.</p>

                        <p>4) Movable hands: The hand can be shooted like a rocket, and you can control its movement
                            and position using the joystick. This technique can help people fetching objects that are
                            far away from them.</p>
                        <p>User studies will be conducted to evaluate these techniques further.</p>

                    </div>
                </div>
            </div>
            <div id="voiding-the-touch" style="margin-top: 7rem;">
                <!-- Title row -->
                <span class="w3-tag w3-blue">Research</span>
                <div class="w3-row">
                    <h2 class="secTitle "><strong>Voiding the Touch</strong><span class="secTitle2 w3-opacity"> |
                            Postponed by COVID-19</span></h2>
                    <h2 class="subTitleCS ">Fengyuan Zhu, <strong class=" w3-text-blue">Zhuoyue
                            Lyu</strong>, Tovi Grossman<span class="w3-opacity"> | Aiming
                            at <strong>Conference on Human Factors in Computing Systems (CHI) 2022</strong></span>
                    </h2>
                </div>
                <!-- video row -->
                <div class="w3-row">
                    <div class="w3-twothird" style="margin-top: 1rem;">
                        <img src="./cs_files/voiding-unclear.png" alt="Me" class="w3-image w3-card-2 " width="95%">
                        <div style="display: flex; width: 95%;">
                            <div style="flex: 2;margin-top: 1rem;">
                                <div class="video_wrapper w3-card-4">
                                    <iframe class="iframe " src="https://www.youtube.com/embed/7C8-askR-vk"
                                        frameborder="0 "
                                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture "
                                        allowfullscreen></iframe>
                                </div>
                            </div>
                            <div style="flex: 1.42;margin-top: 1rem;">
                                <img src="./cs_files/voiding-setup.jpg" class="w3-image w3-card-4"
                                    style=" margin-bottom: 1rem;">
                            </div>
                        </div>
                    </div>
                    <div class="bodyText w3-third" style="margin-top: 1rem;">
                        <a>
                            While there seems to be promise in leveraging 2D mobile devices for Virtual Reality (VR)
                            input, there is an interesting research question regarding how well users can target
                            items on a physical touch screen when their hand is not rendered. VR is a unique
                            platform when the hands of the users are not inherently visible. Without such rendering,
                            they are forced to rely on proprioception alone, until contact is made, at which point
                            they can rely on visual feedback of the touch screen. As such, we first conducted a
                            study looking at the human’s ability to target when their finger is not visible. An
                            essential factor in such a scenario is the use of “body-relative” interaction – where
                            there is a real-world physical frame of reference. As such, we tested three
                            conditions – where the user was holding the phone and tapping with their thumb on the
                            same hand (greatest spatial reference); user was holding the phone and tapping with the
                            opposite hand’s index finger (medium spatial reference); the phone was mounted and
                            accessed with the index finger (least spatial reference)
                        </a>
                        <p>
                            -----------------------------<br>
                            * The preview image on the left is blurred on purpose, since this work has
                            not been published yet.
                        </p>
                    </div>
                </div>
                <!-- <p>Understanding performance of touch events on a physical device under different
                                input modalities in virtual environments by analyzing time and accuracy data
                                from user studies. </p> -->
            </div>
            <div id="voiding-the-touch" style="margin-top: 7rem;">
                <!-- Title row -->
                <span class="w3-tag w3-green">Work</span>
                <div class="w3-row">
                    <h2 class="secTitle "><strong>IBM Watson Studio</strong><span class="secTitle2 w3-opacity"> |
                            Software Engineer (UI)</span></h2>
                    <!-- <h2 class="subTitleCS w3-opacity ">Designer: Dillon Eversman, Voranouth Supadulya</h2> -->
                </div>
                <div class="w3-row">
                    <div class="w3-third" style="margin-top: 1rem;">
                        <div class="w3-center" style="max-width: 95%;">
                            <img src="./cs_files/IBM-1.png" class="w3-image w3-card-2" style=" margin-bottom: 0.25rem;">
                            <h2 class="subTitleCS">Developer: <strong class=" w3-text-blue">Zhuoyue
                                    Lyu</strong>, Alexander French</h2>
                            <img src="./cs_files/IBM-3.png" class="w3-image w3-card-2" style=" margin-bottom: 0.25rem;">
                            <h2 class="subTitleCS">Developer: <strong class=" w3-text-blue">Zhuoyue
                                    Lyu</strong></h2>
                        </div>
                    </div>
                    <div class="w3-third" style="margin-top: 1rem;">
                        <div class="w3-center" style="max-width: 95%;">
                            <img src="./cs_files/IBM-2.png" class="w3-image w3-card-2"
                                style=" margin-bottom: 0.25rem; ">
                            <h2 class="subTitleCS">Developer: <strong class=" w3-text-blue">Zhuoyue
                                    Lyu</strong>, Albert Weon Jun Oh</h2>
                            <img src="./cs_files/IBM-4.png" class="w3-image w3-card-2"
                                style=" margin-bottom: 0.25rem; width: 95% !important;">
                            <h2 class="subTitleCS">Developer: Nicholas Mazzitelli, <strong class=" w3-text-blue">Zhuoyue
                                    Lyu</strong></h2>
                            <img src="./cs_files/IBM-8.png" class="w3-image w3-card-2"
                                style=" margin-bottom: 0.25rem; width: 95% !important;">
                            <h2 class="subTitleCS">Developer: <strong class=" w3-text-blue">Zhuoyue
                                    Lyu</strong>, Alexander French, Nicholas Mazzitelli, Shuangfan Gao</h2>
                        </div>
                    </div>
                    <div class="w3-third" style="margin-top: 1rem;">
                        <div class="w3-center" style="max-width: 95%;">
                            <img src="./cs_files/IBM-5.png" class="w3-image w3-card-2"
                                style=" margin-bottom: 0.25rem;width: 95% !important;">
                            <h2 class="subTitleCS">Developer: <strong class=" w3-text-blue">Zhuoyue
                                    Lyu</strong></h2>
                            <img src="./cs_files/IBM-6.png" class="w3-image w3-card-2"
                                style=" margin-bottom: 0.25rem;width: 95% !important;">
                            <h2 class="subTitleCS">Developer: Nicholas Mazzitelli, <strong class=" w3-text-blue">Zhuoyue
                                    Lyu</strong></h2>
                            <img src="./cs_files/IBM-7.png" class="w3-image w3-card-2"
                                style=" margin-bottom: 0.25rem;width: 95% !important;">
                            <h2 class="subTitleCS">Developer: <strong class=" w3-text-blue">Zhuoyue
                                    Lyu</strong>, Nicholas Mazzitelli</h2>
                        </div>
                    </div>
                </div>
            </div>
            <footer class="w3-container w3-padding-1 w3-center w3-opacity w3-white " style="margin-top: 5rem">
                <p class="w3-medium ">Copyright © 2021 Zhuoyue Lyu. All rights reserved.
                    <a href="mailto:zhuoyue.lyu@mail.utoronto.ca " target="_top ">
                </p>
            </footer>
        </div>
    </div>
</body>

</html>